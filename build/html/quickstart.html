

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quickstart - Segmenting Data and Generating Networks &mdash; NetTracer3D 0.6.9 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=084304ff"></script>
      <script src="_static/doctools.js?v=9bcbadda"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Labeling Branches and Branch Networks" href="branches.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            NetTracer3D
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">User Guide:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quickstart - Segmenting Data and Generating Networks</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#launch-nettracer3d">Launch NetTracer3D</a></li>
<li class="toctree-l2"><a class="reference internal" href="#interface-overview">Interface Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#the-control-panel">The Control Panel</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loading-an-image">Loading an Image</a></li>
<li class="toctree-l2"><a class="reference internal" href="#basic-navigation-of-the-image-viewer-window">Basic Navigation of the Image Viewer Window</a></li>
<li class="toctree-l2"><a class="reference internal" href="#segmenting-data">Segmenting Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#using-the-machine-learning-segmenter">Using the Machine Learning Segmenter</a></li>
<li class="toctree-l3"><a class="reference internal" href="#denoising-the-segmentation">Denoising the Segmentation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#generating-a-network-using-edges">Generating a Network Using Edges</a></li>
<li class="toctree-l2"><a class="reference internal" href="#generating-a-network-based-on-proximity">Generating a Network Based on Proximity</a></li>
<li class="toctree-l2"><a class="reference internal" href="#exporting-data">Exporting Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#using-network-and-or-image-data-in-python">Using Network (And/Or Image) data in python</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="branches.html">Labeling Branches and Branch Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="properties.html">Properties of a Network3D Object</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Detailed Function Guide:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="right_clicking.html">Right Clicking - All Options Available by Right Clicking in the GUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="file_menu.html">All File Menu Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="analyze_menu.html">All Analyze Menu Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="process_menu.html">All Process Menu Options</a></li>
<li class="toctree-l1"><a class="reference internal" href="image_menu.html">All Image Menu Options</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contact.html">Contact</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="roadmap.html">Road Map</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NetTracer3D</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Quickstart - Segmenting Data and Generating Networks</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/quickstart.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quickstart-segmenting-data-and-generating-networks">
<span id="quickstart"></span><h1>Quickstart - Segmenting Data and Generating Networks<a class="headerlink" href="#quickstart-segmenting-data-and-generating-networks" title="Link to this heading"></a></h1>
<p>This guide will help you get started with NetTracer3D by walking through a simple example.</p>
<section id="launch-nettracer3d">
<h2>Launch NetTracer3D<a class="headerlink" href="#launch-nettracer3d" title="Link to this heading"></a></h2>
<p>After <a class="reference internal" href="installation.html"><span class="doc">Installation</span></a>, you can launch NetTracer3D from the command line:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nettracer3d
</pre></div>
</div>
<p>This will open the main application window.</p>
</section>
<section id="interface-overview">
<h2>Interface Overview<a class="headerlink" href="#interface-overview" title="Link to this heading"></a></h2>
<a class="reference internal image-reference" href="_images/interface_overview.png"><img alt="NetTracer3D Interface Overview" src="_images/interface_overview.png" style="width: 800px;" />
</a>
<p>The NetTracer3D interface consists of:</p>
<ul class="simple">
<li><p>(Left) Main Visualization Area: The image viewer window where the 3D stack is displayed as 2D slices.</p></li>
<li><p>(Bottom) Control Panel: Widgets for quick interaction with the image viewer window.</p></li>
<li><p>(Top Right) Tabulated Data: Where data tables from analysis will be placed.</p></li>
<li><p>(Bottom Right) Network Data: Where paired nodes in your network will be organized.</p></li>
<li><p>(Top) Menu Bar: Options to load/export data and run analysis.</p></li>
</ul>
<p>In addition, since it will be usually run out of the command window, be sure to check your command window for printed updates about what NetTracer3D is actually doing.</p>
</section>
<section id="the-control-panel">
<h2>The Control Panel<a class="headerlink" href="#the-control-panel" title="Link to this heading"></a></h2>
<a class="reference internal image-reference" href="_images/control_panel.png"><img alt="Control Panel Overview" src="_images/control_panel.png" style="width: 800px;" />
</a>
<p>Before we start with an example, we’ll go over the control panel on the bottom. It includes the following widgets:</p>
<ol class="arabic simple">
<li><dl class="simple">
<dt>The Active Image widget</dt><dd><ul class="simple">
<li><p>Clicking on the carrot will allow you to select which image is ‘Active’. Many processing/analysis functions will by default run on the image that is ‘Active’. Furthermore, when clicking or drawing in the Image Viewer Window, the ‘Active’ image is the one that will be referenced.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>The zoom widget (magnifying glass - Shortcut Z)</dt><dd><ul class="simple">
<li><p>Press z or click the magnifying glass widget to enter the zoom mode. Clicking the Image Viewer Window in zoom mode will cause you to zoom in. Right clicking will cause you to zoom out.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>The pan widget (hand - Shortcut middle mouse)</dt><dd><ul class="simple">
<li><p>Press middle mouse button or click the hand widget to enter pan mode. Use the mouse to drag along the Image Viewer Window while in pan mode to move around the image.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>The highlight overlay display widget (eye - Shortcut X)</dt><dd><ul class="simple">
<li><p>Press x or click the eye widget to toggle whether the highlight overlay is visible. Clicking on objects in the node/edges channels (and certain other functions) will generate a yellow highlight atop the image viewer window that denotes what is selected.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>The image markup widget (pen)</dt><dd><ul class="simple">
<li><dl class="simple">
<dt>Click the pen widget to enter the image markup mode. While in this mode, clicking on the active image will write values of 255 directly into the image data where you are clicking.</dt><dd><ul>
<li><dl class="simple">
<dt>While in pen mode, the following additional functionalities are offered:</dt><dd><ol class="arabic simple">
<li><p>Left click will erase any positive data and write 0 directly into the image.</p></li>
<li><p>Ctrl + Mouse Wheel will enlarge the draw/erase area.</p></li>
<li><p>Press F to swap to a fill can. Clicking with the fill can will write the val 255 into the entirety of any background (0 value) areas in your image that are connected to the clicked point. While in fill can mode only, ctrl+z will undo the most recent action.</p></li>
<li><p>Press D in either pen or fill can mode to enable the 3D version of these tools. The 3D pen will draw along several image stacks at once. The number of stacks above you are drawing on is indicated by the number above the 3D pen (i.e. a value of 5 will write into the current stack, 2 above, and 2 below). Use the mousewheel to enlarge or decrease this number. The 3D fill can will fill the entirety of a 3D hole (which can be the entire image background if not careful). Like the 2D fill can, use ctrl+z while still in the fill can mode to undo the last fill-can action.</p></li>
</ol>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>The threshold/segment widget (pencil)</dt><dd><ul class="simple">
<li><p>Click the pencil widget to open the menu to either Threshold or use Machine-Learning segmentation. Please see the Threshold/Segment guide for more information.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>The channel widgets (Nodes, Edges, Overlay1, Overlay2)</dt><dd><ul class="simple">
<li><p>Click the channel widgets to toggle whether the channel is visible. The ‘x’ widget located next to the channel buttons will prompt if you want to delete that channel or not.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>The scrollbar.</dt><dd><ul class="simple">
<li><p>The knob at the center of the scroll bar can be moved with the mouse to scroll through the 3D image stack. Use the left or right arrows on either side to scroll one frame at a time. Shift + mouse wheel can also be used to scroll through the stack. Ctrl + Shift + Mouse Wheel will result in a faster scroll.</p></li>
</ul>
</dd>
</dl>
</li>
</ol>
</section>
<section id="loading-an-image">
<h2>Loading an Image<a class="headerlink" href="#loading-an-image" title="Link to this heading"></a></h2>
<p>To load an image, select File -&gt; Load. You will see the following options:</p>
<ol class="arabic simple">
<li><p>Load Network3D Object</p></li>
<li><p>Load Nodes</p></li>
<li><p>Load Edges</p></li>
<li><p>Load Overlay 1</p></li>
<li><p>Load Overlay 2</p></li>
<li><p>Load Network</p></li>
<li><p>Load Misc Properties</p></li>
</ol>
<p>Options 2-5 correspond the the four image viewing channels that are supported in NetTracer3D.
Whenever you are beginning with a new image that you would like to segment, load it into the nodes channel with ‘Load Nodes’.
This will prompt you to browse for an image in the .tif/.tiff (for microscopic data), .nii (for medical images), or .jpg/.png file formats.</p>
<p><strong>Note that if your image has real value scaling (ie microns per pixel), those will not automatically populate and should be assigned in ‘Image -&gt; Properties’ before any processing occurs.</strong></p>
<p>We will begin by loading this cartoon-rendition of a slime mold as an example:</p>
<a class="reference internal image-reference" href="_images/slime.png"><img alt="Slime Mold Render" src="_images/slime.png" style="width: 500px;" />
</a>
<p>We will use File -&gt; load nodes. Since we are loading an RBG image in this case, we say ‘yes’ to the prompt asking if this is a color image so it can be converted to grayscale.</p>
<a class="reference internal image-reference" href="_images/slime_gui.png"><img alt="Slime Mold in GUI" src="_images/slime_gui.png" style="width: 800px;" />
</a>
<p>Note the Image Viewer Window shows nodes-images through a red filter by default, although this can be changed.</p>
</section>
<section id="basic-navigation-of-the-image-viewer-window">
<h2>Basic Navigation of the Image Viewer Window<a class="headerlink" href="#basic-navigation-of-the-image-viewer-window" title="Link to this heading"></a></h2>
<p>The Image Viewer Window is able to display the four channels in addition to a highlight overlay:
These channels include:</p>
<ul class="simple">
<li><p>Nodes: Contain the image that represents the objects you would like to group together in a network. Loads images in grayscale.</p></li>
<li><p>Edges: Contain the image that may be used as a reference when grouping the objects in the node image together. Furthermore, the branch-labeling algorithms will be executed on the edge image. Loads images in grayscale.</p></li>
<li><p>Overlay1: Displays an optional overlay. (Supports color images)</p></li>
<li><p>Overlay2: Displays a second optional overlay. (Supports color images)</p></li>
</ul>
<p>The highlight overlay is a special image that is used to convey selected objects to the user.</p>
<p>If you are having trouble seeing your image data, use ‘Image -&gt; Adjust Brightness/Contrast’ to modify the brightness of each channel.</p>
<p>Only the images in the Nodes and Edges channels may be interacted with in the Image Viewer Window.
This will additionally only occur for Nodes when ‘Nodes’ is selected as the active image, or similarly for Edges when ‘Edges’ is selective.
In such cases, clicking on an object in the window will select all elements in the corresponding image that contain that numerical value (for example, clicking on a pixel/voxel of grayscale val 1 will select all pixel/voxels containing the value 1).</p>
<ul class="simple">
<li><p>This selection will be denoted in the highlight overlay, and information about it will be presented in the tabulated data widget in the top right.</p></li>
<li><p>Selected nodes/edges will also be bolded and highlighted in the network table widget on the bottom right.</p></li>
<li><p>Selected objects may furthermore have specific functions run on them specifically. Many of these options can be seen by right clicking in the Image Viewer Window. For more information, see right click options.</p></li>
<li><p>Clicking the background (val 0) will deselect all objects.</p></li>
<li><p>To select additional objects, hold ctrl + click to select a new object while maintaining the previous selection.</p></li>
<li><p>To select multiple objects at once, click and drag in the image viewer window to create a selection (also supports ctrl + click).</p></li>
</ul>
</section>
<section id="segmenting-data">
<span id="segmenting"></span><h2>Segmenting Data<a class="headerlink" href="#segmenting-data" title="Link to this heading"></a></h2>
<p>Most of the algorithms in NetTracer3D expect either binary images (where all values are 0 or positive), or labeled images, which are grayscale images that have grouped objects into distinct labels (ie 1, 2, 3, etc).
Presegmented data can be used directly, however our data is not yet segmented, so we will use NetTracer3D’s segmentation options to process it.</p>
<p>To begin a segmentation, click on the pencil widget. You will be prompted with the following window:</p>
<a class="reference internal image-reference" href="_images/segthresh_menu.png"><img alt="Segmentation Menu" src="_images/segthresh_menu.png" style="width: 200px;" />
</a>
<p>Execution mode can be used to select to segment by either direct intensity thresholding (for when SNR is already good or to sort out specific labels), or volume based thresholding (for already segmented images - to sort out noise or select a specific range of objects).</p>
<ul class="simple">
<li><p>Press select to open a corresponding threshold window.</p></li>
</ul>
<p>Machine Learning can be used to segment by feature morphology in the image. This is a more general use case that can be used on most types of images to create a segmentation. In this case, we will select this option.</p>
<section id="using-the-machine-learning-segmenter">
<h3>Using the Machine Learning Segmenter<a class="headerlink" href="#using-the-machine-learning-segmenter" title="Link to this heading"></a></h3>
<p>Machine Learning segmentation can be executed on any image in the nodes channel. It will require use of Overlay1, Overlay2, and the highlight overlay. Because of this, it is recommended to segment images in seperate sessions as processing binary data.</p>
<a class="reference internal image-reference" href="_images/ml_seg.png"><img alt="Machine Learning Menu" src="_images/ml_seg.png" style="width: 800px;" />
</a>
<p>Referencing the above image, the ML segmentation uses the following options:</p>
<ul class="simple">
<li><p>Nodes: Contains the data of the image we are segmenting.</p></li>
<li><p>Overlay1: Contains the training data.</p></li>
<li><p>Overlay2: Will contain the final segmentation once it’s been generated.</p></li>
<li><p>Highlight Overlay: Will display segmentation previews.</p></li>
<li><p>Brush Widget (Drawing Tools): Click to enter the brush mode. This works similar to the pen mode described above, but without 3D/Fill Can features. Click with the brush in the Image Viewer Window to mark what objects we want to keep (Foreground) and what objects we want to exclude (Background). This training data will be written directly into Overlay1 (with vals of 1 representing the foreground and 2 the background). Right click can be used to erase these markings. Use ctrl + mouse wheel to enlarge/shrink the brush.</p></li>
<li><p>Foreground (Drawing Tool): Select to have the brush mark foreground (denoted by green markings). Press a to toggle with background.</p></li>
<li><p>Background (Drawing Tools): Select to have the brush mark background (denoted by red markings). Press a to toggle with foreground.</p></li>
<li><p>Train by 2D Slice Patterns (Processing Options): When selected, the model will be trained using 2D feature maps.</p></li>
<li><p>Train by 3D Patterns (Processing Options): When selected, the model will be trained using 3D feature maps.</p></li>
<li><p>Train Quick Model (Training): Click to train the model to segment your image based on the regions selected in your training data.</p></li>
<li><p>Train More Detailed Model (Training): Does the same as above but has additional feature training.</p></li>
<li><p>Preview Segment (Segmentation): When clicked, the model will begin segmenting your image as a preview, without interrupting the current training session. Use this to assess the current state of the model to decide if it needs additional training. This preview will be displayed in the highlight overlay, with foreground denoted by yellow and background denoted by blue.</p></li>
<li><p>Segment All (Segmentation): When clicked (after a warning), the training session will pause to segment the entire image with the current model. It is recommended that you save your images before doing this (File -&gt; Save Network 3D Object As), in case the segmentation needs to be interrupted (It can only be paused by terminating the program). When finished, the binary segmentation will be placed in Overlay 2.</p></li>
</ul>
<a class="reference internal image-reference" href="_images/seg_example.png"><img alt="Machine Learning Segmentation Example" src="_images/seg_example.png" style="width: 800px;" />
</a>
<p><em>Above: Example ML-Segmentation in process. I have marked foreground with green markings and background with red markings. The yellow regions have been selected by the current model as foreground, and the blue as background. Longer training sessions will produce more specific segmentation results.</em></p>
<p>Saving results:</p>
<ul class="simple">
<li><p>To save the resulting segmentation, use ‘File -&gt; Save Overlay2 As’. To save the training data (to reuse or retrain the model later), use ‘File -&gt; Save Overlay1 As’.</p></li>
<li><p>‘File -&gt; Save Network3D Object As’ can be used to save all images together.</p></li>
</ul>
<p>We will save the above segmentation to be used as nodes in our network.</p>
</section>
<section id="denoising-the-segmentation">
<h3>Denoising the Segmentation<a class="headerlink" href="#denoising-the-segmentation" title="Link to this heading"></a></h3>
<p>Although we have succesfully segmented our image, there are examples of noise that have slipped through. Luckily, NetTracer3D offers several options to clean up binary segmentations.
In this instance, we will be using volume thresholding to clean up noise.</p>
<ol class="arabic simple">
<li><p>First, I load a new instance of NetTracer3D and load my new binary slime mold segmentation into the nodes channel.</p></li>
<li><p>Next, I click the pencil widget, change the ‘Execution Mode’ to ‘Using Volumes’, and choose ‘Select’ to open the Volume Threshold Window.</p></li>
<li><p>When prompted, I allow the system to ‘label’ my nodes (Assign each binary object a distinct numerical value).</p></li>
</ol>
<a class="reference internal image-reference" href="_images/volthresh.png"><img alt="Volume Segmentation Example" src="_images/volthresh.png" style="width: 800px;" />
</a>
<p><a href="#id1"><span class="problematic" id="id2">*</span></a>Above: De-noising with the Volume Thresholder. I use the red bar to exclude the small objects. On the left, the objects I stand to keep are shown in yellow.</p>
<p>Segmenting with the Volume Thresholder is simple. The displayed histogram represents the distribution of volumes of objects in my image. The red bar can be dragged to exclude objects with low volumes while the blue bar can be dragged to exclude objects with high volumes.</p>
<ul class="simple">
<li><p>Alternatively, I can manually enter the minimum/maximum values to retain.</p></li>
<li><p>Pressing Preview will have the Highlight Overlay show which objects we are including in yellow.</p></li>
<li><p>Pressing apply threshold will segment the image as shown in the preview.</p></li>
</ul>
<p>I apply this threshold and finally use ‘Process -&gt; Image -&gt; Fill Holes’ to fill any holes in my binary image, resulting in the following finished segmentation:</p>
<a class="reference internal image-reference" href="_images/final_seg.png"><img alt="Slime Mold Segmentation" src="_images/final_seg.png" style="width: 800px;" />
</a>
</section>
</section>
<section id="generating-a-network-using-edges">
<h2>Generating a Network Using Edges<a class="headerlink" href="#generating-a-network-using-edges" title="Link to this heading"></a></h2>
<p>Similar to the nodes, we segment the edges from the image (I did not show this process but it would be the same steps as above, just selecting the connection regions rather than the nodes). This is the image through which the nodes will be connected. (In imaging this will often be on a different channel, such as an overlapping image of nerves or vessels. In this demo, we use the same image for both).
We load these in with ‘File -&gt; Load Edges’ and select ‘Process -&gt; Calculate -&gt; Calculate Connectivity Network’.</p>
<a class="reference internal image-reference" href="_images/segment_prenetwork.png"><img alt="Slime Mold Prenetwork" src="_images/segment_prenetwork.png" style="width: 800px;" />
</a>
<p>We enter the following params and execute the network generation:</p>
<a class="reference internal image-reference" href="_images/connectivity_network_menu.png"><img alt="Connectivity Network Menu" src="_images/connectivity_network_menu.png" style="width: 800px;" />
</a>
<p><em>In short, these params are currently telling the nodes to ‘search’ 30 pixels outwards (Node search param) for edges to connect to. Nodes that share an edge will be connected in the resultant network. For more information on using this algorithm, see</em> <a class="reference internal" href="process_menu.html#connectivity-network"><span class="std std-ref">‘Process -&gt; Calculate -&gt; Calculate Connectivity Network…’</span></a></p>
<p>This yields the following network:</p>
<a class="reference internal image-reference" href="_images/connectivity_network.png"><img alt="Connectivity Network" src="_images/connectivity_network.png" style="width: 800px;" />
</a>
<p>The nodes/edges images are tweaked based on how the network-search param used them (to ensure consistency with the output). In Overlay1, we have generated a binary overlay displaying the direct network connections (white lines). In the bottom right table, we can see the IDs of the linked nodes in the first two columns, and the ID of each pair’s associated edge in the third column.
In this case, the majority of nodes are joined through a large hub-edge in the center, while nodes along the sides have less direct connections.
To elucidate the connectivity here, we can convert this central edge trunk into a single node using ‘Process -&gt; Modify Network’ as shown:</p>
<a class="reference internal image-reference" href="_images/modifying.png"><img alt="Modifying Network" src="_images/modifying.png" style="width: 800px;" />
</a>
<p>This results in the trunk becoming a node. The resultant image with network overlay alongside a graph of the network is shown below:</p>
<a class="reference internal image-reference" href="_images/final_connectivity_network.png"><img alt="Final Network" src="_images/final_connectivity_network.png" style="width: 800px;" />
</a>
<p><em>This network was displayed using the ‘Analyze -&gt; Show Network’ option while selecting louvain community detection</em></p>
<p>If we load back in the original image with the overlay, we can see how the image information has been compressed to a set of connected integers.</p>
<a class="reference internal image-reference" href="_images/final_demo.png"><img alt="Final Demo Connection Img" src="_images/final_demo.png" style="width: 800px;" />
</a>
<p>While an image like this would not be too hard to manually label, imagine doing this for thousands or tens of thousands of nodes throughout a 3D image.
This is where NetTracer3D shines! For example, the image below is one such usage where I created neural networks between groups of glomeruli in the human kidney from 3D lightsheet images I captured:</p>
<a class="reference internal image-reference" href="_images/5x_mothergloms.png"><img alt="Example Network" src="_images/5x_mothergloms.png" style="width: 800px;" />
</a>
</section>
<section id="generating-a-network-based-on-proximity">
<h2>Generating a Network Based on Proximity<a class="headerlink" href="#generating-a-network-based-on-proximity" title="Link to this heading"></a></h2>
<p>We will go over another simple example of creating networks. This one is even easier, as it only requires nodes to function, and simply groups nodes as connected pairs based on their distance to each other.
Open a new instance of NetTracer3D and once more load in the binary segmentation of the nodes that was created above.
First, use ‘Process -&gt; Image -&gt; Label Objects’ to assign each binary object a unique label.
Next, select ‘Process -&gt; Calculate -&gt; Calculate Proximity Network’.</p>
<a class="reference internal image-reference" href="_images/proximity_menu.png"><img alt="Proximity Network Menu" src="_images/proximity_menu.png" style="width: 800px;" />
</a>
<p><em>Here we can see the menu to generate proximity networks. The search distance here is set to 300, which means nodes will look 300 pixels out for connections (although this will correspond to your scalings). Note there are two options available for searching, shown in the carrot dropdown next to ‘Execution Mode’. The first option searches from centroids and works quite well with big data as the data structure is far simpler. The second option searches from object borders and may be slower on large images by comparison. In this case, I use the second option since these objects are heterogenously sized. For more information on using this algorithm, see</em> <a class="reference internal" href="process_menu.html#proximity-network"><span class="std std-ref">‘Process -&gt; Calculate -&gt; Calculate Proximity Network…’</span></a></p>
<p>And after algorithm execution:</p>
<a class="reference internal image-reference" href="_images/proximity.png"><img alt="Proximity Network" src="_images/proximity.png" style="width: 800px;" />
</a>
<a class="reference internal image-reference" href="_images/proximity2.png"><img alt="Proximity Network 2" src="_images/proximity2.png" style="width: 800px;" />
</a>
<p>Proximity networks are a generic way to group together objects in 3D space and are ideal, for example, for grouping together cellular neighborhoods.
One use for such cellular neighborhoods is grouping them into communities and analyzing their composition!</p>
</section>
<section id="exporting-data">
<h2>Exporting Data<a class="headerlink" href="#exporting-data" title="Link to this heading"></a></h2>
<p>Use ‘File -&gt; Save As’ to export any channels that have been generated/edited in NetTracer3D, in the .tif file format.
To bul save, choosing ‘Save As Network3D Object’ will have NetTracer3D dump most of the active data into a new folder in a format it can later reload with ‘Load Network3D Object’.
Regarding tables and networks, they all can be exported with right click as either .csv or .xlsx files for outside analysis. Furthermore, networks feature additional export options for use with other network analysis software, such as Gephi.</p>
<p>NetTracer3D is designed for some amount of end-to-end functionality, however exporting allows support for downstream analysis in other software such as ImageJ or Microsoft Excel.</p>
<section id="using-network-and-or-image-data-in-python">
<h3>Using Network (And/Or Image) data in python<a class="headerlink" href="#using-network-and-or-image-data-in-python" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>NetTracer3D is mainly designed to be interacted via its GUI, but some users may want to extract its properties for direct use in a python script. This is most useful with the network property, which is saved as a networkx graph object, as a user would be able to leverage the entire networkx toolbox for their own analysis pipelines.</p></li>
<li><p>NetTracer3D organizes many of its properties into the ‘Network_3D’ class. The easiest way to export NetTracer3D data into code is to just save the ‘Network_3D’ objects in the GUI, then in python, creating a new ‘Network_3D’ object, loading in its components from the saved data, and calling those properties. For example:</p></li>
<li><p>See <a class="reference external" href="https://networkx.org/">https://networkx.org/</a> for information about using the networkx graph object.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">nettracer3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">nettracer</span> <span class="k">as</span> <span class="n">n3d</span>

<span class="n">my_network</span> <span class="o">=</span> <span class="n">n3d</span><span class="o">.</span><span class="n">Network_3D</span><span class="p">()</span> <span class="c1">#Declare a new Network_3D object</span>
<span class="n">my_network</span><span class="o">.</span><span class="n">load_network</span><span class="p">(</span><span class="n">file_path</span> <span class="o">=</span> <span class="s1">&#39;path/to/my/network/file/that/netracer3d_gui/had/saved/output_network.csv&#39;</span><span class="p">)</span> <span class="c1">#If we just want to load the networkx graph</span>
<span class="n">my_network</span><span class="o">.</span><span class="n">assemble</span><span class="p">(</span><span class="n">directory</span> <span class="o">=</span> <span class="s1">&#39;path/to/my/directory/where/netracer3d_gui/saved/the/network3d_object&#39;</span><span class="p">)</span> <span class="c1">#If we want to load all the properties. Note that this function looks for the files with the names that the &#39;Save (As) Network 3D Object&#39; option assigned them.</span>

<span class="c1">#Using the properties in code directly (Note these will return None if they had not been assigned to anything - ie, if the file used to .assemble() was missing them):</span>
<span class="n">nodes</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">nodes</span> <span class="c1">#The nodes channel data, as a numpy array</span>
<span class="n">edges</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">edges</span> <span class="c1">#The edges channel data, as a numpy array</span>
<span class="n">overlay_1</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">network_overlay</span> <span class="c1">#The overlay1 channel data, as a numpy array</span>
<span class="n">overlay_2</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">id_overlay</span> <span class="c1">#The overlay2 channel data, as a numpy array</span>
<span class="n">network</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">network</span> <span class="c1">#The network data, as a network x graph object</span>
<span class="n">node_centroids</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">node_centroids</span> <span class="c1">#Centroids of nodes, as a python dictionary</span>
<span class="n">edge_centroids</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">edge_centroids</span> <span class="c1">#Centroids of edges, as a python dictionary</span>
<span class="n">node_communities</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">communities</span> <span class="c1">#Communities of nodes, as a python dictionary</span>
<span class="n">node_identities</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">node_identities</span> <span class="c1">#Identities of nodes, as a python dictionary</span>
<span class="n">xy_scale</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">xy_scale</span> <span class="c1">#The dimensional scaling of the flat xy plane that corresponds to the image used to generate this network, as a float.</span>
<span class="n">z_scale</span> <span class="o">=</span> <span class="n">my_network</span><span class="o">.</span><span class="n">z_scale</span> <span class="c1">#The z step size of the 3D stack that corresponds to the image used to generate this network, as a float.</span>

<span class="c1">#If I do something to the above properties, and want to save, the contents of the Network_3D object can be saved with this method:</span>
<span class="n">my_network</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">directory</span> <span class="o">=</span> <span class="s1">&#39;path/to/save/the/outputs&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="next-steps">
<h2>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading"></a></h2>
<p>Once you have a hang on generating the default network types, proceed to the <a class="reference internal" href="branches.html"><span class="doc">Labeling Branches and Branch Networks</span></a> to learn about using NetTracer3D to label branches of objects and create branch networks.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="branches.html" class="btn btn-neutral float-right" title="Labeling Branches and Branch Networks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Liam McLaughlin.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>